{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorium 4\n",
    "### Michał Kurleto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from scipy import sparse\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1. Wyszukiwarka\n",
    "### 1. Przygotowanie danych\n",
    "Przygotuj duży (> 1000 elementów) zbiór dokumentów tekstowych w języku angielskim (np. wybrany korpus tekstów, podzbiór artykułów Wikipedii, zbiór dokumentów HTML uzyskanych za pomocą Web crawlera, zbiór rozdziałów wyciętych z\n",
    "różnych książek)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pliki stworzone z podzielenia książek \"Moby-Dick\" i \"Pride and Prejudice\" na około 500 plikow txt każda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1012"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir('books/')\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przykładowy plik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  “Scarcely had we proceeded two days on the sea, when about sunrise a\n",
      "  great many Whales and other monsters of the sea, appeared. Among the\n",
      "  former, one was of a most monstrous size.... This came towards us,\n",
      "  open-mouthed, raising the waves on all sides, and beating the sea\n",
      "  before him into a foam.” —_Tooke’s Lucian_. “_The True History_.”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  “He visited this country also with a view of catching horse-whales,\n",
      "  which had bones of very great value for their teeth, of which he\n",
      "  brought some to the king.... The best whales were catched in his own\n",
      "  country, of which some were forty-eight, some fifty yards long. He\n",
      "  said that he was one of six who had killed sixty in two days.”\n",
      "  —_Other or Other’s verbal narrative taken down from his mouth by King\n",
      "  Alfred, A.D._ 890.\n",
      "\n",
      "  “And whereas all the other things, whether beast or vessel, that\n",
      "  enter into the dreadful gulf of this monster’s (whale’s) mouth, are\n",
      "  immediately lost and swallowed up, the sea-gudgeon retires into it in\n",
      "  great security, and there sleeps.” —MONTAIGNE. —_Apology for Raimond\n",
      "  Sebond_.\n",
      "\n",
      "  “Let us fly, let us fly! Old Nick take me if is not Leviathan\n",
      "  described by the noble prophet Moses in the life of patient Job.”\n",
      "  —_Rabelais_.\n",
      "\n",
      "  “This whale’s liver was two cartloads.” —_Stowe’s Annals_.\n",
      "\n",
      "  “The great Leviathan that maketh the seas to seethe like boiling\n",
      "  pan.” —_Lord Bacon’s Version of the Psalms_.\n",
      "\n",
      "  “Touching that monstrous bulk of the whale or ork we have received\n",
      "  nothing certain. They grow exceeding fat, insomuch that an incredible\n",
      "  quantity of oil will be extracted out of one whale.” —_Ibid_.\n",
      "  “_History of Life and Death_.”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  “The sovereignest thing on earth is parmacetti for an inward bruise.”\n",
      "  —_King Henry_.\n",
      "\n",
      "  “Very like a whale.” —_Hamlet_.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('books/moby_dick_pt_10.txt','r',encoding=\"utf8\") as file:\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "Określ słownik słów kluczowych (termów) potrzebny do wyznaczenia wektorów\n",
    "cech bag-of-words (indeksacja). Przykładowo zbiorem takim może być unia wszystkich słów występujących we wszystkich tekstach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(files):\n",
    "    dictionary = {}\n",
    "    \n",
    "    for file in files:\n",
    "        with open('books/' + file,'r',encoding=\"utf8\") as file_obj:\n",
    "            contents = file_obj.read()\n",
    "            \n",
    "            words = nltk.word_tokenize(contents)\n",
    "            \n",
    "            #remove all punctuation\n",
    "            words = [word.lower() for word in words if word.isalnum()]\n",
    "            \n",
    "            for word in words:\n",
    "                if word in dictionary:\n",
    "                    dictionary[word] += 1\n",
    "                else:\n",
    "                    dictionary[word] = 1\n",
    "                    \n",
    "    return dictionary.keys(), dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 najpopularniejszych słów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba słów w słowniku: 18706\n",
      "\n",
      "20 najpopularniejszych słów: \n",
      "\n",
      "the: 19088\n",
      "of: 10425\n",
      "and: 10054\n",
      "to: 8864\n",
      "a: 6694\n",
      "in: 6125\n",
      "that: 4559\n",
      "i: 4101\n",
      "it: 3982\n",
      "his: 3767\n",
      "was: 3473\n",
      "he: 3190\n",
      "as: 2918\n",
      "with: 2862\n",
      "not: 2772\n",
      "but: 2760\n",
      "for: 2700\n",
      "is: 2615\n",
      "her: 2507\n",
      "s: 2442\n"
     ]
    }
   ],
   "source": [
    "dictionary, word_quantities = create_dictionary(files)\n",
    "words = sorted(word_quantities.items(), key = lambda x: x[1], reverse=True)\n",
    "print(\"Liczba słów w słowniku: {0}\\n\".format(len(words)))\n",
    "print(\"20 najpopularniejszych słów: \\n\")\n",
    "for i, word in enumerate(words):\n",
    "    if i == 20: break\n",
    "    print(word[0] + ': ' + str(word[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Vector\n",
    "Dla każdego dokumentu j wyznacz wektor cech bag-of-words dj zawierający częstości występowania poszczególnych słów (termów) w tekście."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector(file, dictionary):\n",
    "    vector = {}\n",
    "    \n",
    "    for word in dictionary:\n",
    "        vector[word] = 0\n",
    "    \n",
    "    with open('books/' + file,'r',encoding=\"utf8\") as file_obj:\n",
    "        contents = file_obj.read()\n",
    "\n",
    "    words = nltk.word_tokenize(contents)\n",
    "\n",
    "    #remove all punctuation\n",
    "    words = [word.lower() for word in words if word.isalnum()]\n",
    "\n",
    "    for word in words:\n",
    "        if word in vector:\n",
    "            vector[word] += 1\n",
    "        else:\n",
    "             vector[word] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przykładowy wektor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 17,\n",
       " 'project': 0,\n",
       " 'gutenberg': 0,\n",
       " 'ebook': 0,\n",
       " 'of': 14,\n",
       " 'moby': 0,\n",
       " 'dick': 0,\n",
       " 'or': 3,\n",
       " 'whale': 3,\n",
       " 'by': 2,\n",
       " 'herman': 0,\n",
       " 'melville': 0,\n",
       " 'this': 4,\n",
       " 'is': 2,\n",
       " 'for': 3,\n",
       " 'use': 0,\n",
       " 'anyone': 0,\n",
       " 'anywhere': 0,\n",
       " 'at': 0,\n",
       " 'no': 0,\n",
       " 'cost': 0,\n",
       " 'and': 6,\n",
       " 'with': 1,\n",
       " 'almost': 0,\n",
       " 'restrictions': 0,\n",
       " 'whatsoever': 0,\n",
       " 'you': 0,\n",
       " 'may': 0,\n",
       " 'copy': 0,\n",
       " 'it': 1,\n",
       " 'give': 0,\n",
       " 'away': 0,\n",
       " 'under': 0,\n",
       " 'terms': 0,\n",
       " 'license': 0,\n",
       " 'included': 0,\n",
       " 'online': 0,\n",
       " 'title': 0,\n",
       " 'author': 0,\n",
       " 'release': 0,\n",
       " 'date': 0,\n",
       " 'december': 0,\n",
       " '25': 0,\n",
       " '2008': 0,\n",
       " '2701': 0,\n",
       " 'last': 0,\n",
       " 'updated': 0,\n",
       " '3': 0,\n",
       " '2017': 0,\n",
       " 'language': 0,\n",
       " 'english': 0,\n",
       " 'character': 0,\n",
       " 'set': 0,\n",
       " 'encoding': 0,\n",
       " 'start': 0,\n",
       " 'produced': 0,\n",
       " 'daniel': 0,\n",
       " 'lazarus': 0,\n",
       " 'jonesey': 0,\n",
       " 'david': 0,\n",
       " 'widger': 0,\n",
       " 'contents': 0,\n",
       " 'etymology': 0,\n",
       " 'extracts': 0,\n",
       " 'supplied': 0,\n",
       " 'a': 5,\n",
       " 'chapter': 0,\n",
       " '1': 0,\n",
       " 'loomings': 0,\n",
       " '2': 0,\n",
       " '4': 0,\n",
       " 'counterpane': 0,\n",
       " '5': 0,\n",
       " 'breakfast': 0,\n",
       " '6': 0,\n",
       " 'street': 0,\n",
       " '7': 0,\n",
       " 'chapel': 0,\n",
       " '8': 0,\n",
       " 'pulpit': 0,\n",
       " '9': 0,\n",
       " 'sermon': 0,\n",
       " '10': 0,\n",
       " 'bosom': 0,\n",
       " 'friend': 0,\n",
       " '11': 0,\n",
       " 'nightgown': 0,\n",
       " '12': 0,\n",
       " 'biographical': 0,\n",
       " '13': 0,\n",
       " 'wheelbarrow': 0,\n",
       " '14': 0,\n",
       " 'nantucket': 0,\n",
       " '15': 0,\n",
       " 'chowder': 0,\n",
       " '16': 0,\n",
       " 'ship': 0,\n",
       " '17': 0,\n",
       " 'ramadan': 0,\n",
       " '18': 0,\n",
       " 'his': 2,\n",
       " 'mark': 0,\n",
       " '19': 0,\n",
       " 'prophet': 1,\n",
       " '20': 0,\n",
       " 'all': 2,\n",
       " 'astir': 0,\n",
       " '21': 0,\n",
       " 'going': 0,\n",
       " 'aboard': 0,\n",
       " 'scarcely': 1,\n",
       " 'had': 3,\n",
       " 'we': 2,\n",
       " 'proceeded': 1,\n",
       " 'two': 3,\n",
       " 'days': 1,\n",
       " 'on': 3,\n",
       " 'sea': 3,\n",
       " 'when': 1,\n",
       " 'about': 1,\n",
       " 'sunrise': 1,\n",
       " 'great': 4,\n",
       " 'many': 1,\n",
       " 'whales': 2,\n",
       " 'other': 3,\n",
       " 'monsters': 1,\n",
       " 'appeared': 1,\n",
       " 'among': 1,\n",
       " 'former': 1,\n",
       " 'one': 3,\n",
       " 'was': 3,\n",
       " 'most': 1,\n",
       " 'monstrous': 2,\n",
       " 'size': 1,\n",
       " 'came': 1,\n",
       " 'towards': 1,\n",
       " 'us': 3,\n",
       " 'raising': 1,\n",
       " 'waves': 1,\n",
       " 'sides': 1,\n",
       " 'beating': 1,\n",
       " 'before': 1,\n",
       " 'him': 1,\n",
       " 'into': 3,\n",
       " 's': 7,\n",
       " 'true': 1,\n",
       " 'he': 4,\n",
       " 'visited': 1,\n",
       " 'country': 2,\n",
       " 'also': 1,\n",
       " 'view': 1,\n",
       " 'catching': 1,\n",
       " 'which': 3,\n",
       " 'bones': 1,\n",
       " 'very': 2,\n",
       " 'value': 1,\n",
       " 'their': 1,\n",
       " 'teeth': 1,\n",
       " 'brought': 1,\n",
       " 'some': 3,\n",
       " 'to': 2,\n",
       " 'king': 2,\n",
       " 'best': 1,\n",
       " 'were': 2,\n",
       " 'catched': 1,\n",
       " 'in': 4,\n",
       " 'own': 1,\n",
       " 'fifty': 1,\n",
       " 'yards': 1,\n",
       " 'long': 1,\n",
       " 'said': 1,\n",
       " 'that': 5,\n",
       " 'six': 1,\n",
       " 'who': 1,\n",
       " 'killed': 1,\n",
       " 'sixty': 1,\n",
       " 'verbal': 1,\n",
       " 'narrative': 1,\n",
       " 'taken': 1,\n",
       " 'down': 1,\n",
       " 'from': 1,\n",
       " 'mouth': 2,\n",
       " 'alfred': 1,\n",
       " '890': 1,\n",
       " 'whereas': 1,\n",
       " 'things': 1,\n",
       " 'whether': 1,\n",
       " 'beast': 1,\n",
       " 'vessel': 1,\n",
       " 'enter': 1,\n",
       " 'dreadful': 1,\n",
       " 'gulf': 1,\n",
       " 'monster': 1,\n",
       " 'are': 1,\n",
       " 'immediately': 1,\n",
       " 'lost': 1,\n",
       " 'swallowed': 1,\n",
       " 'up': 1,\n",
       " 'retires': 1,\n",
       " 'security': 1,\n",
       " 'there': 1,\n",
       " 'raimond': 1,\n",
       " 'let': 2,\n",
       " 'fly': 2,\n",
       " 'old': 1,\n",
       " 'nick': 1,\n",
       " 'take': 1,\n",
       " 'me': 1,\n",
       " 'if': 1,\n",
       " 'not': 1,\n",
       " 'leviathan': 2,\n",
       " 'described': 1,\n",
       " 'noble': 1,\n",
       " 'moses': 1,\n",
       " 'life': 2,\n",
       " 'patient': 1,\n",
       " 'liver': 1,\n",
       " 'maketh': 1,\n",
       " 'seas': 1,\n",
       " 'seethe': 1,\n",
       " 'like': 2,\n",
       " 'boiling': 1,\n",
       " 'bacon': 1,\n",
       " 'version': 1,\n",
       " 'touching': 1,\n",
       " 'bulk': 1,\n",
       " 'ork': 1,\n",
       " 'have': 1,\n",
       " 'received': 1,\n",
       " 'nothing': 1,\n",
       " 'certain': 1,\n",
       " 'they': 1,\n",
       " 'grow': 1,\n",
       " 'exceeding': 1,\n",
       " 'fat': 1,\n",
       " 'insomuch': 1,\n",
       " 'an': 2,\n",
       " 'incredible': 1,\n",
       " 'quantity': 1,\n",
       " 'oil': 1,\n",
       " 'will': 1,\n",
       " 'be': 1,\n",
       " 'extracted': 1,\n",
       " 'out': 1,\n",
       " 'sovereignest': 1,\n",
       " 'thing': 1,\n",
       " 'earth': 1,\n",
       " 'parmacetti': 1,\n",
       " 'inward': 1,\n",
       " 'given': 0,\n",
       " 'time': 0,\n",
       " 'next': 0,\n",
       " 'day': 0,\n",
       " 'would': 0,\n",
       " 'certainly': 0,\n",
       " 'sail': 0,\n",
       " 'so': 0,\n",
       " 'morning': 0,\n",
       " 'queequeg': 0,\n",
       " 'i': 0,\n",
       " 'took': 0,\n",
       " 'early': 0,\n",
       " 'nearly': 0,\n",
       " 'o': 0,\n",
       " 'clock': 0,\n",
       " 'but': 0,\n",
       " 'only': 0,\n",
       " 'grey': 0,\n",
       " 'imperfect': 0,\n",
       " 'misty': 0,\n",
       " 'dawn': 0,\n",
       " 'drew': 0,\n",
       " 'nigh': 0,\n",
       " 'wharf': 0,\n",
       " 'sailors': 0,\n",
       " 'running': 0,\n",
       " 'ahead': 0,\n",
       " 'see': 0,\n",
       " 'right': 0,\n",
       " 'can': 0,\n",
       " 't': 0,\n",
       " 'shadows': 0,\n",
       " 'she': 0,\n",
       " 'off': 0,\n",
       " 'guess': 0,\n",
       " 'come': 0,\n",
       " 'avast': 0,\n",
       " 'cried': 0,\n",
       " 'voice': 0,\n",
       " 'whose': 0,\n",
       " 'owner': 0,\n",
       " 'same': 0,\n",
       " 'coming': 0,\n",
       " 'close': 0,\n",
       " 'behind': 0,\n",
       " 'laid': 0,\n",
       " 'hand': 0,\n",
       " 'upon': 0,\n",
       " 'both': 0,\n",
       " 'our': 0,\n",
       " 'shoulders': 0,\n",
       " 'then': 0,\n",
       " 'insinuating': 0,\n",
       " 'himself': 0,\n",
       " 'between': 0,\n",
       " 'stood': 0,\n",
       " 'stooping': 0,\n",
       " 'forward': 0,\n",
       " 'little': 0,\n",
       " 'uncertain': 0,\n",
       " 'twilight': 0,\n",
       " 'strangely': 0,\n",
       " 'peering': 0,\n",
       " 'elijah': 0,\n",
       " 'hands': 0,\n",
       " 'lookee': 0,\n",
       " 'here': 0,\n",
       " 'shaking': 0,\n",
       " 'go': 0,\n",
       " 'way': 0,\n",
       " 'ain': 0,\n",
       " 'yes': 0,\n",
       " 'what': 0,\n",
       " 'business': 0,\n",
       " 'yours': 0,\n",
       " 'do': 0,\n",
       " 'know': 0,\n",
       " 'consider': 0,\n",
       " 'impertinent': 0,\n",
       " 'wasn': 0,\n",
       " 'aware': 0,\n",
       " 'slowly': 0,\n",
       " 'wonderingly': 0,\n",
       " 'looking': 0,\n",
       " 'unaccountable': 0,\n",
       " 'glances': 0,\n",
       " 'oblige': 0,\n",
       " 'my': 0,\n",
       " 'withdrawing': 0,\n",
       " 'indian': 0,\n",
       " 'pacific': 0,\n",
       " 'oceans': 0,\n",
       " 'prefer': 0,\n",
       " 'ye': 0,\n",
       " 'back': 0,\n",
       " 'afore': 0,\n",
       " 'cracked': 0,\n",
       " 'holloa': 0,\n",
       " 'stationary': 0,\n",
       " 'hailing': 0,\n",
       " 'removed': 0,\n",
       " 'few': 0,\n",
       " 'paces': 0,\n",
       " 'never': 0,\n",
       " 'mind': 0,\n",
       " 'stole': 0,\n",
       " 'again': 0,\n",
       " 'suddenly': 0,\n",
       " 'clapping': 0,\n",
       " 'shoulder': 0,\n",
       " 'did': 0,\n",
       " 'anything': 0,\n",
       " 'men': 0,\n",
       " 'while': 0,\n",
       " 'ago': 0,\n",
       " 'struck': 0,\n",
       " 'plain': 0,\n",
       " 'question': 0,\n",
       " 'answered': 0,\n",
       " 'saying': 0,\n",
       " 'thought': 0,\n",
       " 'four': 0,\n",
       " 'five': 0,\n",
       " 'too': 0,\n",
       " 'dim': 0,\n",
       " 'once': 0,\n",
       " 'more': 0,\n",
       " 'quitted': 0,\n",
       " 'softly': 0,\n",
       " 'after': 0,\n",
       " 'find': 0,\n",
       " 'em': 0,\n",
       " 'now': 0,\n",
       " 'rejoined': 0,\n",
       " 'moving': 0,\n",
       " 'oh': 0,\n",
       " 'warn': 0,\n",
       " 'family': 0,\n",
       " 'frost': 0,\n",
       " 'shan': 0,\n",
       " 'soon': 0,\n",
       " 'unless': 0,\n",
       " 'grand': 0,\n",
       " 'these': 0,\n",
       " 'words': 0,\n",
       " 'finally': 0,\n",
       " 'departed': 0,\n",
       " 'leaving': 0,\n",
       " 'moment': 0,\n",
       " 'small': 0,\n",
       " 'wonderment': 0,\n",
       " 'frantic': 0,\n",
       " 'impudence': 0,\n",
       " 'stepping': 0,\n",
       " 'board': 0,\n",
       " 'pequod': 0,\n",
       " 'found': 0,\n",
       " 'everything': 0,\n",
       " 'profound': 0,\n",
       " 'quiet': 0,\n",
       " 'soul': 0,\n",
       " 'cabin': 0,\n",
       " 'entrance': 0,\n",
       " 'locked': 0,\n",
       " 'within': 0,\n",
       " 'hatches': 0,\n",
       " 'lumbered': 0,\n",
       " 'coils': 0,\n",
       " 'rigging': 0,\n",
       " 'forecastle': 0,\n",
       " 'slide': 0,\n",
       " 'scuttle': 0,\n",
       " 'open': 0,\n",
       " 'seeing': 0,\n",
       " 'light': 0,\n",
       " 'went': 0,\n",
       " 'rigger': 0,\n",
       " 'wrapped': 0,\n",
       " 'tattered': 0,\n",
       " 'thrown': 0,\n",
       " 'whole': 0,\n",
       " 'length': 0,\n",
       " 'chests': 0,\n",
       " 'face': 0,\n",
       " 'downwards': 0,\n",
       " 'inclosed': 0,\n",
       " 'folded': 0,\n",
       " 'arms': 0,\n",
       " 'profoundest': 0,\n",
       " 'slumber': 0,\n",
       " 'slept': 0,\n",
       " 'those': 0,\n",
       " 'saw': 0,\n",
       " 'where': 0,\n",
       " 'gone': 0,\n",
       " 'dubiously': 0,\n",
       " 'sleeper': 0,\n",
       " 'seemed': 0,\n",
       " 'noticed': 0,\n",
       " 'alluded': 0,\n",
       " 'hence': 0,\n",
       " 'myself': 0,\n",
       " 'been': 0,\n",
       " 'optically': 0,\n",
       " 'deceived': 0,\n",
       " 'matter': 0,\n",
       " 'otherwise': 0,\n",
       " 'inexplicable': 0,\n",
       " 'beat': 0,\n",
       " 'marking': 0,\n",
       " 'jocularly': 0,\n",
       " 'hinted': 0,\n",
       " 'perhaps': 0,\n",
       " 'sit': 0,\n",
       " 'body': 0,\n",
       " 'telling': 0,\n",
       " 'establish': 0,\n",
       " 'accordingly': 0,\n",
       " 'put': 0,\n",
       " 'rear': 0,\n",
       " 'as': 0,\n",
       " 'though': 0,\n",
       " 'feeling': 0,\n",
       " 'soft': 0,\n",
       " 'enough': 0,\n",
       " 'without': 0,\n",
       " 'ado': 0,\n",
       " 'sat': 0,\n",
       " 'quietly': 0,\n",
       " 'gracious': 0,\n",
       " 'don': 0,\n",
       " 'perry': 0,\n",
       " 'dood': 0,\n",
       " 'seat': 0,\n",
       " 'won': 0,\n",
       " 'hurt': 0,\n",
       " 'call': 0,\n",
       " 'benevolent': 0,\n",
       " 'countenance': 0,\n",
       " 'how': 0,\n",
       " 'hard': 0,\n",
       " 'breathes': 0,\n",
       " 'heaving': 0,\n",
       " 'get': 0,\n",
       " 'heavy': 0,\n",
       " 'grinding': 0,\n",
       " 'poor': 0,\n",
       " 'look': 0,\n",
       " 'll': 0,\n",
       " 'twitch': 0,\n",
       " 'wonder': 0,\n",
       " 'just': 0,\n",
       " 'beyond': 0,\n",
       " 'head': 0,\n",
       " 'lighted': 0,\n",
       " 'tomahawk': 0,\n",
       " 'pipe': 0,\n",
       " 'feet': 0,\n",
       " 'kept': 0,\n",
       " 'passing': 0,\n",
       " 'over': 0,\n",
       " 'meanwhile': 0,\n",
       " 'questioning': 0,\n",
       " 'broken': 0,\n",
       " 'fashion': 0,\n",
       " 'gave': 0,\n",
       " 'understand': 0,\n",
       " 'land': 0,\n",
       " 'owing': 0,\n",
       " 'absence': 0,\n",
       " 'settees': 0,\n",
       " 'sofas': 0,\n",
       " 'sorts': 0,\n",
       " 'chiefs': 0,\n",
       " 'people': 0,\n",
       " 'generally': 0,\n",
       " 'custom': 0,\n",
       " 'fattening': 0,\n",
       " 'lower': 0,\n",
       " 'orders': 0,\n",
       " 'ottomans': 0,\n",
       " 'furnish': 0,\n",
       " 'house': 0,\n",
       " 'comfortably': 0,\n",
       " 'respect': 0,\n",
       " 'buy': 0,\n",
       " 'eight': 0,\n",
       " 'ten': 0,\n",
       " 'lazy': 0,\n",
       " 'fellows': 0,\n",
       " 'lay': 0,\n",
       " 'them': 0,\n",
       " 'round': 0,\n",
       " 'piers': 0,\n",
       " 'alcoves': 0,\n",
       " 'besides': 0,\n",
       " 'convenient': 0,\n",
       " 'excursion': 0,\n",
       " 'much': 0,\n",
       " 'better': 0,\n",
       " 'than': 0,\n",
       " 'convertible': 0,\n",
       " 'occasion': 0,\n",
       " 'chief': 0,\n",
       " 'calling': 0,\n",
       " 'attendant': 0,\n",
       " 'desiring': 0,\n",
       " 'make': 0,\n",
       " 'settee': 0,\n",
       " 'spreading': 0,\n",
       " 'tree': 0,\n",
       " 'damp': 0,\n",
       " 'marshy': 0,\n",
       " 'place': 0,\n",
       " 'narrating': 0,\n",
       " 'every': 0,\n",
       " 'flourished': 0,\n",
       " 'easy': 0,\n",
       " 'wild': 0,\n",
       " 'reminiscences': 0,\n",
       " 'its': 0,\n",
       " 'uses': 0,\n",
       " 'brained': 0,\n",
       " 'foes': 0,\n",
       " 'soothed': 0,\n",
       " 'directly': 0,\n",
       " 'attracted': 0,\n",
       " 'sleeping': 0,\n",
       " 'strong': 0,\n",
       " 'vapor': 0,\n",
       " 'completely': 0,\n",
       " 'filling': 0,\n",
       " 'contracted': 0,\n",
       " 'hole': 0,\n",
       " 'began': 0,\n",
       " 'tell': 0,\n",
       " 'breathed': 0,\n",
       " 'sort': 0,\n",
       " 'muffledness': 0,\n",
       " 'troubled': 0,\n",
       " 'nose': 0,\n",
       " 'revolved': 0,\n",
       " 'twice': 0,\n",
       " 'rubbed': 0,\n",
       " 'eyes': 0,\n",
       " 'smokers': 0,\n",
       " 'shipped': 0,\n",
       " 'does': 0,\n",
       " 'aye': 0,\n",
       " 'her': 0,\n",
       " 'sails': 0,\n",
       " 'captain': 0,\n",
       " 'indeed': 0,\n",
       " 'ask': 0,\n",
       " 'further': 0,\n",
       " 'questions': 0,\n",
       " 'concerning': 0,\n",
       " 'ahab': 0,\n",
       " 'heard': 0,\n",
       " 'noise': 0,\n",
       " 'deck': 0,\n",
       " 'starbuck': 0,\n",
       " 'lively': 0,\n",
       " 'mate': 0,\n",
       " 'good': 0,\n",
       " 'man': 0,\n",
       " 'pious': 0,\n",
       " 'alive': 0,\n",
       " 'must': 0,\n",
       " 'turn': 0,\n",
       " 'followed': 0,\n",
       " 'clear': 0,\n",
       " 'crew': 0,\n",
       " 'twos': 0,\n",
       " 'threes': 0,\n",
       " 'riggers': 0,\n",
       " 'bestirred': 0,\n",
       " 'themselves': 0,\n",
       " 'mates': 0,\n",
       " 'actively': 0,\n",
       " 'engaged': 0,\n",
       " 'several': 0,\n",
       " 'shore': 0,\n",
       " 'busy': 0,\n",
       " 'bringing': 0,\n",
       " 'various': 0,\n",
       " 'remained': 0,\n",
       " 'invisibly': 0,\n",
       " 'enshrined': 0,\n",
       " '22': 0,\n",
       " 'merry': 0,\n",
       " 'christmas': 0,\n",
       " 'noon': 0,\n",
       " 'final': 0,\n",
       " 'dismissal': 0,\n",
       " 'hauled': 0,\n",
       " 'charity': 0,\n",
       " 'stubb': 0,\n",
       " 'second': 0,\n",
       " 'spare': 0,\n",
       " 'bible': 0,\n",
       " 'captains': 0,\n",
       " 'peleg': 0,\n",
       " 'bildad': 0,\n",
       " 'issued': 0,\n",
       " 'turning': 0,\n",
       " 'sure': 0,\n",
       " 'spoke': 0,\n",
       " 'got': 0,\n",
       " 'eh': 0,\n",
       " 'well': 0,\n",
       " 'muster': 0,\n",
       " 'aft': 0,\n",
       " 'need': 0,\n",
       " 'profane': 0,\n",
       " 'however': 0,\n",
       " 'hurry': 0,\n",
       " 'thee': 0,\n",
       " 'point': 0,\n",
       " 'starting': 0,\n",
       " 'voyage': 0,\n",
       " 'high': 0,\n",
       " 'appearances': 0,\n",
       " 'port': 0,\n",
       " 'sign': 0,\n",
       " 'yet': 0,\n",
       " 'seen': 0,\n",
       " 'idea': 0,\n",
       " 'presence': 0,\n",
       " 'means': 0,\n",
       " 'necessary': 0,\n",
       " 'getting': 0,\n",
       " 'weigh': 0,\n",
       " 'steering': 0,\n",
       " 'proper': 0,\n",
       " 'pilot': 0,\n",
       " 'stayed': 0,\n",
       " 'below': 0,\n",
       " 'natural': 0,\n",
       " 'especially': 0,\n",
       " 'merchant': 0,\n",
       " 'service': 0,\n",
       " 'show': 0,\n",
       " 'considerable': 0,\n",
       " 'anchor': 0,\n",
       " 'remain': 0,\n",
       " 'table': 0,\n",
       " 'having': 0,\n",
       " 'farewell': 0,\n",
       " 'friends': 0,\n",
       " 'quit': 0,\n",
       " 'chance': 0,\n",
       " 'think': 0,\n",
       " 'talking': 0,\n",
       " 'commanding': 0,\n",
       " 'sons': 0,\n",
       " 'bachelors': 0,\n",
       " 'lingered': 0,\n",
       " 'mr': 0,\n",
       " 'drive': 0,\n",
       " 'strike': 0,\n",
       " 'tent': 0,\n",
       " 'order': 0,\n",
       " 'whalebone': 0,\n",
       " 'marquee': 0,\n",
       " 'pitched': 0,\n",
       " 'except': 0,\n",
       " 'thirty': 0,\n",
       " 'years': 0,\n",
       " 'known': 0,\n",
       " 'capstan': 0,\n",
       " 'blood': 0,\n",
       " 'thunder': 0,\n",
       " 'command': 0,\n",
       " 'sprang': 0,\n",
       " 'handspikes': 0,\n",
       " 'station': 0,\n",
       " 'occupied': 0,\n",
       " 'part': 0,\n",
       " 'addition': 0,\n",
       " 'officers': 0,\n",
       " 'licensed': 0,\n",
       " 'pilots': 0,\n",
       " 'being': 0,\n",
       " 'suspected': 0,\n",
       " 'made': 0,\n",
       " 'save': 0,\n",
       " 'ships': 0,\n",
       " 'concerned': 0,\n",
       " 'piloted': 0,\n",
       " 'any': 0,\n",
       " 'say': 0,\n",
       " 'might': 0,\n",
       " 'bows': 0,\n",
       " 'approaching': 0,\n",
       " 'intervals': 0,\n",
       " 'singing': 0,\n",
       " 'dismal': 0,\n",
       " 'stave': 0,\n",
       " 'psalmody': 0,\n",
       " 'cheer': 0,\n",
       " 'windlass': 0,\n",
       " 'roared': 0,\n",
       " 'forth': 0,\n",
       " 'chorus': 0,\n",
       " 'girls': 0,\n",
       " 'booble': 0,\n",
       " 'alley': 0,\n",
       " 'hearty': 0,\n",
       " 'nevertheless': 0,\n",
       " 'three': 0,\n",
       " 'previous': 0,\n",
       " 'told': 0,\n",
       " 'songs': 0,\n",
       " 'allowed': 0,\n",
       " 'particularly': 0,\n",
       " 'sister': 0,\n",
       " 'placed': 0,\n",
       " 'choice': 0,\n",
       " 'watts': 0,\n",
       " 'each': 0,\n",
       " 'seaman': 0,\n",
       " 'berth': 0,\n",
       " 'meantime': 0,\n",
       " 'overseeing': 0,\n",
       " 'ripped': 0,\n",
       " 'swore': 0,\n",
       " 'astern': 0,\n",
       " 'frightful': 0,\n",
       " 'manner': 0,\n",
       " 'sink': 0,\n",
       " 'could': 0,\n",
       " 'involuntarily': 0,\n",
       " 'paused': 0,\n",
       " 'handspike': 0,\n",
       " 'thinking': 0,\n",
       " 'perils': 0,\n",
       " 'ran': 0,\n",
       " 'such': 0,\n",
       " 'devil': 0,\n",
       " 'comforting': 0,\n",
       " 'salvation': 0,\n",
       " 'spite': 0,\n",
       " 'seven': 0,\n",
       " 'hundred': 0,\n",
       " 'felt': 0,\n",
       " 'sudden': 0,\n",
       " 'sharp': 0,\n",
       " 'poke': 0,\n",
       " 'horrified': 0,\n",
       " 'apparition': 0,\n",
       " 'act': 0,\n",
       " 'leg': 0,\n",
       " 'immediate': 0,\n",
       " 'vicinity': 0,\n",
       " 'first': 0,\n",
       " 'kick': 0,\n",
       " 'heave': 0,\n",
       " 'marchant': 0,\n",
       " 'spring': 0,\n",
       " 'thou': 0,\n",
       " 'break': 0,\n",
       " 'thy': 0,\n",
       " 'backbone': 0,\n",
       " 'why': 0,\n",
       " 'quohog': 0,\n",
       " 'chap': 0,\n",
       " 'red': 0,\n",
       " 'whiskers': 0,\n",
       " 'green': 0,\n",
       " 'pants': 0,\n",
       " 'your': 0,\n",
       " 'moved': 0,\n",
       " 'along': 0,\n",
       " 'using': 0,\n",
       " 'freely': 0,\n",
       " 'imperturbable': 0,\n",
       " 'leading': 0,\n",
       " 'thinks': 0,\n",
       " 'drinking': 0,\n",
       " 'something': 0,\n",
       " 'glided': 0,\n",
       " 'short': 0,\n",
       " 'cold': 0,\n",
       " 'northern': 0,\n",
       " 'merged': 0,\n",
       " 'night': 0,\n",
       " 'ourselves': 0,\n",
       " 'broad': 0,\n",
       " 'wintry': 0,\n",
       " 'ocean': 0,\n",
       " 'freezing': 0,\n",
       " 'spray': 0,\n",
       " 'cased': 0,\n",
       " 'ice': 0,\n",
       " 'polished': 0,\n",
       " 'armor': 0,\n",
       " 'rows': 0,\n",
       " 'bulwarks': 0,\n",
       " 'glistened': 0,\n",
       " 'moonlight': 0,\n",
       " 'white': 0,\n",
       " 'ivory': 0,\n",
       " 'tusks': 0,\n",
       " 'huge': 0,\n",
       " 'elephant': 0,\n",
       " 'vast': 0,\n",
       " 'curving': 0,\n",
       " 'icicles': 0,\n",
       " 'depended': 0,\n",
       " 'lank': 0,\n",
       " 'headed': 0,\n",
       " 'watch': 0,\n",
       " 'ever': 0,\n",
       " 'anon': 0,\n",
       " 'craft': 0,\n",
       " 'deep': 0,\n",
       " 'dived': 0,\n",
       " 'sent': 0,\n",
       " 'shivering': 0,\n",
       " 'winds': 0,\n",
       " 'howled': 0,\n",
       " 'cordage': 0,\n",
       " 'rang': 0,\n",
       " 'steady': 0,\n",
       " 'notes': 0,\n",
       " 'sweet': 0,\n",
       " 'fields': 0,\n",
       " 'swelling': 0,\n",
       " 'flood': 0,\n",
       " 'stand': 0,\n",
       " 'dressed': 0,\n",
       " 'living': 0,\n",
       " 'jews': 0,\n",
       " 'canaan': 0,\n",
       " 'jordan': 0,\n",
       " 'rolled': 0,\n",
       " 'sound': 0,\n",
       " 'sweetly': 0,\n",
       " 'full': 0,\n",
       " 'hope': 0,\n",
       " 'fruition': 0,\n",
       " 'frigid': 0,\n",
       " 'winter': 0,\n",
       " 'boisterous': 0,\n",
       " 'atlantic': 0,\n",
       " 'wet': 0,\n",
       " 'wetter': 0,\n",
       " 'jacket': 0,\n",
       " 'pleasant': 0,\n",
       " 'haven': 0,\n",
       " 'store': 0,\n",
       " 'meads': 0,\n",
       " 'glades': 0,\n",
       " 'eternally': 0,\n",
       " 'vernal': 0,\n",
       " 'grass': 0,\n",
       " 'shot': 0,\n",
       " 'untrodden': 0,\n",
       " 'unwilted': 0,\n",
       " 'remains': 0,\n",
       " 'midsummer': 0,\n",
       " 'gained': 0,\n",
       " 'offing': 0,\n",
       " 'needed': 0,\n",
       " 'longer': 0,\n",
       " 'stout': 0,\n",
       " 'accompanied': 0,\n",
       " 'ranging': 0,\n",
       " 'alongside': 0,\n",
       " 'curious': 0,\n",
       " 'unpleasing': 0,\n",
       " 'affected': 0,\n",
       " 'juncture': 0,\n",
       " 'loath': 0,\n",
       " 'depart': 0,\n",
       " 'leave': 0,\n",
       " 'bound': 0,\n",
       " 'perilous': 0,\n",
       " 'stormy': 0,\n",
       " 'capes': 0,\n",
       " 'thousands': 0,\n",
       " 'earned': 0,\n",
       " 'dollars': 0,\n",
       " 'invested': 0,\n",
       " 'shipmate': 0,\n",
       " 'sailed': 0,\n",
       " 'encounter': 0,\n",
       " 'terrors': 0,\n",
       " 'pitiless': 0,\n",
       " 'jaw': 0,\n",
       " 'brimful': 0,\n",
       " 'interest': 0,\n",
       " 'paced': 0,\n",
       " 'anxious': 0,\n",
       " 'strides': 0,\n",
       " 'speak': 0,\n",
       " 'another': 0,\n",
       " 'word': 0,\n",
       " 'looked': 0,\n",
       " 'windward': 0,\n",
       " 'wide': 0,\n",
       " 'endless': 0,\n",
       " 'waters': 0,\n",
       " 'bounded': 0,\n",
       " 'unseen': 0,\n",
       " 'eastern': 0,\n",
       " 'continents': 0,\n",
       " 'aloft': 0,\n",
       " 'left': 0,\n",
       " 'everywhere': 0,\n",
       " 'nowhere': 0,\n",
       " 'mechanically': 0,\n",
       " 'coiling': 0,\n",
       " 'rope': 0,\n",
       " 'pin': 0,\n",
       " 'convulsively': 0,\n",
       " 'grasped': 0,\n",
       " 'holding': 0,\n",
       " 'lantern': 0,\n",
       " 'gazing': 0,\n",
       " 'heroically': 0,\n",
       " 'philosopher': 0,\n",
       " 'philosophy': 0,\n",
       " 'tear': 0,\n",
       " 'twinkling': 0,\n",
       " 'eye': 0,\n",
       " 'near': 0,\n",
       " 'run': 0,\n",
       " 'turned': 0,\n",
       " 'comrade': 0,\n",
       " 'boat': 0,\n",
       " 'ahoy': 0,\n",
       " 'careful': 0,\n",
       " 'luck': 0,\n",
       " 'hot': 0,\n",
       " 'supper': 0,\n",
       " 'smoking': 0,\n",
       " 'hurrah': 0,\n",
       " 'god': 0,\n",
       " 'bless': 0,\n",
       " 'holy': 0,\n",
       " 'keeping': 0,\n",
       " 'murmured': 0,\n",
       " 'incoherently': 0,\n",
       " 'fine': 0,\n",
       " 'weather': 0,\n",
       " 'sun': 0,\n",
       " 'needs': 0,\n",
       " 'plenty': 0,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_vector('moby_dick_pt_10.txt',dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "Zbuduj rzadką macierz wektorów cech term-by-document matrix w której wektory cech ułożone są kolumnowo Am×n = [d1|d2| . . . |dn] (m jest liczbą termów w\n",
    "słowniku, a n liczbą dokumentów)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse(files, dictionary):\n",
    "    sparse = []\n",
    "    for file in files:\n",
    "        vector = create_vector(file, dictionary)\n",
    "        sparse.append(vector)\n",
    "    return sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = create_sparse(files, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\n",
    " Przetwórz wstępnie otrzymany zbiór danych mnożąc elementy bag-of-words przez\n",
    "inverse document frequency. Operacja ta pozwoli na redukcję znaczenia często występujących słów.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nw(word, sparse):\n",
    "    counter = 0\n",
    "    for row in sparse:\n",
    "        if row.get(word) != 0:\n",
    "            counter += 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_by_IDF(dictionary, sparse):\n",
    "    for word in dictionary:\n",
    "        nw = get_nw(word, sparse)\n",
    "        if nw == 0:\n",
    "            idf = 0\n",
    "        else:\n",
    "            idf = float(math.log(len(matrix)/nw))\n",
    "        for row in sparse:\n",
    "            row[word] = row.get(word) * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multiply_by_IDF(dictionary, matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\n",
    "Napisz program pozwalający na wprowadzenie zapytania (w postaci sekwencji\n",
    "słów) przekształcanego następnie do reprezentacji wektorowej q (bag-of-words).\n",
    "Program ma zwrócić k dokumentów najbardziej zbliżonych do podanego zapytania q. Użyj korelacji między wektorami jako miary podobieństwa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_from_text(text, dictionary):\n",
    "    vector = {}\n",
    "\n",
    "    for word in dictionary:\n",
    "        vector[word] = 0\n",
    "    \n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    #remove all punctuation\n",
    "    words = [word.lower() for word in words if word.isalnum()]\n",
    "    \n",
    "    for word in words: \n",
    "        if word in vector:\n",
    "            vector[word] += 1\n",
    "    return list(vector.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_documents(text, k, dictionary, sparse):\n",
    "    text_vector = create_vector_from_text(text, dictionary)\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in enumerate(sparse):\n",
    "        cos = np.matmul(np.array(text_vector).T, np.array(list(row.values()))) / (np.linalg.norm(text_vector) * np.linalg.norm(list(row.values())))\n",
    "        if len(results)  < k:\n",
    "            results.append((idx, cos))\n",
    "        else:\n",
    "            if cos > results[0][1] or math.isnan(results[0][1]):\n",
    "                results[0] = (idx, cos)\n",
    "        results = sorted(results, key = lambda x : x[1], reverse=True)\n",
    "    return results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"“Scarcely had we proceeded two days on the sea, when about sunrise a great many Whales and other monsters of the sea, appeared. Among the former, one was of a most monstrous size.... This came towards us, open-mouthed, raising the waves on all sides, and beating the sea before him into a foam.” —_Tooke’s Lucian_. “_The True History\"\n",
    "results = get_closest_documents(text, 5, dictionary, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.26183941261482957),\n",
       " (4, 0.045440766596732),\n",
       " (3, 0.036436620766064916),\n",
       " (0, 0.0024059993034086366),\n",
       " (1, 0.001281909599287602)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wynik wyszukania jest poprawny, pierwszy znaleziony plik jest tym z którego pochodzi fragment tekstu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. \n",
    "Zastosuj normalizację wektorów cech dj i wektora q, tak aby miały one długość 1.\n",
    "Użyj zmodyfikowanej miary podobieństwa otrzymując\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    results = []\n",
    "    for i, row in enumerate(matrix):\n",
    "        results.append(normalize([list(row.values())], norm=\"l1\"))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_matrix = normalize_matrix(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_vector(text, dictionary):\n",
    "    normalized_vector = normalize([create_vector_from_text(text, dictionary)],norm=\"l1\")\n",
    "    results = []\n",
    "    for i, row in enumerate(normal_matrix):\n",
    "        cos = np.matmul(np.array(normalized_vector[0]).T,np.array(row[0]))/(np.linalg.norm(normalized_vector)*np.linalg.norm(row))\n",
    "        results.append((i, cos))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_documents_normalized(text, k, dictionary):\n",
    "    cos_vector = normalize_vector(text, dictionary)\n",
    "    cos_vector = sorted(cos_vector,key=lambda x: x[1], reverse=True)\n",
    "    return cos_vector[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_result = get_closest_documents_normalized(text, 5, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.2618394126148296), (271, 0.0913418953770854), (476, 0.09090090693396023), (121, 0.08856100190466853), (169, 0.08075602643879543)]\n"
     ]
    }
   ],
   "source": [
    "print(normalized_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na pierwszym miejscu znowu pojawia się poprawny wynik, reszta wyników jest inna niż podczas wyszukiwania bez normalizacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.\n",
    "W celu usunięcia szumu z macierzy A (normalized_matrix) zastosuj SVD i low rank approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_and_low_rank(k):\n",
    "    A = []\n",
    "    for row in normal_matrix:\n",
    "        A.append(row[0])\n",
    "    u, s, vt = scipy.sparse.linalg.svds(np.array(A), k=k)\n",
    "    return u @ np.diag(s) @ vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_vectors_svd(text, k, dictionary):\n",
    "    result = svd_and_low_rank(k)\n",
    "    vector_normalized = normalize([create_vector_from_text(text, dictionary)],norm=\"l1\")\n",
    "    results = []\n",
    "    for i, row in enumerate(result):\n",
    "        cos = np.matmul(np.array(vector_normalized[0]).T,np.array(row))/(np.linalg.norm(vector_normalized)*np.linalg.norm(row))\n",
    "        results.append((i, cos))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_documents_svd(text, k, approx_k, dictionary):\n",
    "    vectors = normalize_vectors_svd(text, approx_k, dictionary)\n",
    "    vectors = sorted(vectors, key=lambda x: x[1], reverse=True)\n",
    "    return vectors[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_result = get_closest_documents_svd(text, 3, 150, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.19775867082380671), (272, 0.19565453956129214), (270, 0.18421393274061776)]\n"
     ]
    }
   ],
   "source": [
    "print(svd_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po usunięciu szumu wyniki z wyjątkiem pierwszego zmieniły się."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. \n",
    "Porównaj działanie programu bez usuwania szumu i z usuwaniem szumu. Dla jakiej wartości k wyniki wyszukiwania są najlepsze (subiektywnie). Zbadaj wpływ\n",
    "przekształcenia IDF na wyniki wyszukiwania.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bez odszumiania:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.2618394126148296), (271, 0.0913418953770854), (476, 0.09090090693396023), (121, 0.08856100190466853), (169, 0.08075602643879543)]\n"
     ]
    }
   ],
   "source": [
    "print(normalized_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z odszumianiem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  5\n",
      "[(95, 0.1835704530909705), (361, 0.18351963822253747), (102, 0.18349224332568406)]\n",
      "k =  55\n",
      "[(104, 0.19491507374623), (270, 0.1938096185897583), (412, 0.19064561643560102)]\n",
      "k =  105\n",
      "[(272, 0.19818032685245535), (270, 0.19213667721713745), (2, 0.19010093646985907)]\n",
      "k =  155\n",
      "[(2, 0.19868172487374658), (272, 0.19300849137239195), (270, 0.18505428309683472)]\n",
      "k =  205\n",
      "[(2, 0.22090516790822062), (270, 0.17880071961691596), (128, 0.17522969406947023)]\n",
      "k =  255\n",
      "[(2, 0.22693811838555875), (128, 0.16927521156466918), (271, 0.16428984334488336)]\n",
      "k =  305\n",
      "[(2, 0.24457066232968025), (128, 0.16606258090746112), (476, 0.15680469258794957)]\n",
      "k =  355\n",
      "[(2, 0.24877768434056236), (128, 0.162665909397546), (476, 0.1543960926013011)]\n",
      "k =  405\n",
      "[(2, 0.25838356306697474), (128, 0.15602631904678074), (476, 0.14805037625530715)]\n",
      "k =  455\n",
      "[(2, 0.26902426785496947), (476, 0.14904123933726782), (99, 0.14321389824918246)]\n"
     ]
    }
   ],
   "source": [
    "for k in range(5,500,50):\n",
    "    print(\"k = \",k)\n",
    "    print(get_closest_documents_svd(text, 3, k, dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla dla k >= 155 zaczynami otrzymać poprawny wynik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wnioski:\n",
    "Wyniki z odszumianiem są bardziej trafne niż bez odszumiania, jednak koszt obliczania SVD dla dużych macierzy może być odczuwalny. Przekształcenie IDF pozwala na redukcję znaczenia słów, które występują wielokrotnie w dokumentach, przez co wyniki są bardziej konkretne."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
